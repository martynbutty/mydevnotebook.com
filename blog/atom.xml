<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://www.mydevnotebook.com/blog</id>
    <title>My Dev Notebook Blog</title>
    <updated>2023-08-15T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://www.mydevnotebook.com/blog"/>
    <subtitle>My Dev Notebook Blog</subtitle>
    <icon>https://www.mydevnotebook.com/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Aurora Serverless Backup Configuration]]></title>
        <id>https://www.mydevnotebook.com/blog/2023/08/15/Aurora-backups</id>
        <link href="https://www.mydevnotebook.com/blog/2023/08/15/Aurora-backups"/>
        <updated>2023-08-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Considerations and costs of backups of an Aurora database cluster]]></summary>
        <content type="html"><![CDATA[<p>In a previous article <a href="https://www.mydevnotebook.com/blog/2023/08/01/Aurora-Serverless-database-selection">Aurora Serverless database selection</a>, I discussed the high level considerations around selecting or upgrading to an Aurora server-less V2 database.</p>
<p>If you decided that Aurora V2 is the way to go for your needs, one important but sometimes overlooked aspect is disaster recovery (DR), and in particular the backup configuration of your database. The following discusses some of the main points for configuring backups of an Aurora server-less V2 database.</p>
<p><img src="https://www.mydevnotebook.com/assets/images/error-something-went-wrong-8092487098b5820bbb9bd17dcc99d757.jpg" width="500" height="376"></p>
<h2 id="aurora-backups">Aurora backups</h2>
<p>AWS offers automated continuous backups from one to thirty five days. This period is known as the <strong>retention period</strong>.</p>
<p>Automated backups store the information required to be able to restore the cluster to any point in time during the retention window. They are incremental and charged based on the amount of storage required for this restoration ability.</p>
<p>Database cluster snapshots are always a full backup of the cluster at the time it was taken.</p>
<p>You can retain data outside of the retention period by taking a manual snapshot. Manual snapshots do not expire, but they do incur extra costs as they are not included in the free storage allowance (see below).</p>
<h2 id="costs-of-backups">Costs of backups</h2>
<p>There is an amount of free storage (a.k.a. free tier) for backups:</p>
<ul>
<li>There's no charge for a one day retention period</li>
<li>There's free storage (of backups) up to the value of the cluster volume size (<code>VolumeBytesUsed</code> in cloudwatch metrics).</li>
<li>There's free unlimited storage for all snapshots that lie within the retention period (including manual snapshots).</li>
</ul>
<p>The backup storage includes all the incremental records stored to allow a restore to any point within the retention period. The minimum storage required will therefore always be the cluster size just before the retention period.</p>
<p>The total billed usage for automated backups will never exceed the cumulative cluster volume size over the retention period. For example, with a five day retention period of a 10GB cluster that didn't increase in size over the five days, you would never exceed 50GB (5 days X 10GB).</p>
<p>For databases with lots of changes, the size of the automated backup grows over time. If a database slows or stops growing (i.e. stops experiencing changes), the size of the automated backups will decrease as the stored changes leave the retention window.</p>
<p>Manual snapshots, and AWS backup snapshots are chargeable when they fall outside of the retention period.</p>
<h3 id="cost-examples">Cost examples</h3>
<ul>
<li>Database start cluster size: 100GB</li>
<li>Retention period: 10 days</li>
<li>Daily database growth: 5GB ^1</li>
</ul>
<p>Calculated automated backup storage: <code>100GB [initial volume size] + (10 X 5GB) [size of incremental records] = 150GB total backup usage</code></p>
<p>Subtract the free tier Database cluster volume size at end: 125GB ^2</p>
<p>Billed storage: <code>150GB [total storage required] - 125GB [latest volume size] = 25GB billed usage</code></p>
<p>^1 It's probably unrealistic to assume any database would grow by the same amount every day, but I have here just to make the calculations simpler to understand</p>
<p>^2 Simplified assumption that all inserts, updates and deletes result in this final volume size.</p>
<p>You are billed <strong>per GB month</strong>, as a weighted average over the month. Therefore if you used 100GB over 15 days and 0B for the other 15 days you'd be charged: <code>((100GB X 15 days) + (0B X 15 days)) / 30 = 50GB-month</code></p>
<p>At the time of writing, backup costs $0.021 per GB-month, so the above example would cost just $1.05 for the backup storage.</p>
<h2 id="useful-metrics">Useful metrics</h2>
<table><thead><tr><th>Metric</th><th>Description</th></tr></thead><tbody><tr><td><code>BackupRetentionPeriodStorageUsed</code></td><td>storage in bytes of cluster plus changes for the retention period.</td></tr><tr><td><code>SnapshotStorageUsed</code></td><td>backup storage used in bytes for manual snapshots beyond the retention period of the automated backup period</td></tr><tr><td><code>TotalBackupStorageBilled</code></td><td>the <code>BackupRetentionPeriodStorageUsed</code> + <code>SnapshotStorageUsed</code> - <code>free tier</code>. (free tier will be the latest recorded size of the cluster)</td></tr></tbody></table>
<p>To get an idea of database growth, you can use AWS cost explorer:</p>
<ul>
<li>Granularity: daily</li>
<li>Services: RDS</li>
<li>Usage type group: RDS: storage</li>
<li>Tag: product -&gt; team/product name</li>
</ul>
<p><img src="https://www.mydevnotebook.com/assets/images/DB-size-from-aws-cost-explorer-442e6f336bd88eb558670fdd7c5d6f52.png" width="907" height="273"></p>
<h2 id="backtrack">Backtrack</h2>
<p>Worthy of mention is <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html">Backtrack</a>. Backtrack allows you to "rewind" the DB to a specified time. Backtrack should NOT be used instead of backups, but to complement backups.</p>
<p>Backtrack will only be available if it was enabled when your cluster was created, and has a limit of 72 hours as the max backtrack window. There is additional costs associated with backtrack.</p>
<p>For more info, please follow the link above.</p>
<h2 id="summary">Summary</h2>
<p>Your first step to configuring backups is to understand your database and application.</p>
<p>If a erroneous change was made to you data or schema that didn't get noticed for a few days, would you want to be able to restore to just before that point in time, or attempt to forward fix the data?</p>
<p>How large is your database and how much does it grow each month?</p>
<p>The combination of the above two points, should allow you to come up with some ballpark cost calculations, which should allow you to find a sweet spot of backup retention period vs cost optimisation.</p>
<p>Be cautious of selecting a backup retention period that optimises on cost of backups. If you had to spend a few days with multiple people trying to manually fix your data, that could easily (and significantly) outweigh any saving on the cost of backup storage.</p>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aurora Serverless Database Selection]]></title>
        <id>https://www.mydevnotebook.com/blog/2023/08/01/Aurora-Serverless-database-selection</id>
        <link href="https://www.mydevnotebook.com/blog/2023/08/01/Aurora-Serverless-database-selection"/>
        <updated>2023-08-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How to decide if an Aurora serverless database is right for you]]></summary>
        <content type="html"><![CDATA[<p>With the end of life of MySQL 5.7, you may be considering upgrading to Aurora serverless V2. Here I'll briefly discuss database selection, focussing on Aurora serverless V2 considerations and cluster configuration for production use. Aurora serverless V2 is a way of running a MySQL database without having to worry about the compute required for the predicted load.</p>
<p><img alt="Data Store Image" src="https://www.mydevnotebook.com/assets/images/jan-antonin-kolar-unsplash-1bd21dd64fefd2462850fb91ec2c371b.jpg" width="500" height="347"></p>
<p>Photo by <a href="https://unsplash.com/@jankolar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Jan Antonin Kolar</a> on unsplash.com</p>
<h2 id="to-sql-or-not-to-sql">To SQL or not to SQL</h2>
<p>If you are changing the DB for your application, you should first consider <strong>if a relational database is still the best datastore for your application</strong>. For example, you may get cost, scalability and performance benefits by using a DynamoDB instead.</p>
<p>Migrating to a non-relational DB is a larger piece of work, so you need to consider your future plans for your application and involve your product counterpart in your planning. You should try to identify tradeoffs, benefits and potential problems with the different approaches.</p>
<h2 id="to-serverless-or-not-to-serverless">To serverless, or not to serverless</h2>
<p>If moving database technology has been discounted, then your next consideration is whether a serverless database is the right choice for your application.</p>
<p>Spend some time to understand your database workloads, usage and access patterns. If you have a consistent access pattern with fairly predictable traffic volumes that do not alter much, then Aurora serverless may not be the best solution. In this case, a provisioned RDS instance running on a suitably sized DB instance class will likely prove to be more cost effective.</p>
<p>If your workload is not predictable, especially if you have a low baseline of traffic with regular but unpredictable spikes of activity, then Aurora serverless may be a better fit. It will be more cost effective as it will tick over with just enough resource for your baseline traffic, then be able to quickly scale up to match spikes of traffic.</p>
<h2 id="aurora-v2-main-considerations">Aurora V2 main considerations</h2>
<p>The two main considerations for setting up an Aurora instance is the availability, and the allowed resources.</p>
<h3 id="availability">Availability</h3>
<p>Do you need high availability? If your app is not business critical, then you may be ok with a single writer instance. For business critical applications, it makes more sense to have a multi AZ instance. This is achieved by having a <strong>reader instance</strong> in addition to the writer instance. The reader instance will be in a different AZ to the writer.</p>
<p>With a reader instance, Aurora can promote the reader instance within one minute. This provides the shortest possible downtime, and zero downtime in some cases. The following are examples of downtime that can be mitigated by running a reader instance:</p>
<ul>
<li>AZ containing the writer instance goes offline</li>
<li>Writer instance out of memory crash and restart</li>
<li>Writer instance maintenance window (e.g. for auto minor version upgrades)</li>
</ul>
<p>A reader instance also provides the ability to distribute your workload more effectively, allowing your writer instance to handle the transactions while the reader takes care of read only queries.</p>
<p>It's beyond the scope of this document, but you may also consider RDS proxy, especially if your database gets traffic from lambda functions as it can help prevent swamping the connections to the DB.</p>
<blockquote>
<p>Note: your data is stored in multiple availability zones (AZ's) in a single region by default so is considered highly available. However you won't be able to access your data if you run a single writer instance which is currently down for some reason.</p>
</blockquote>
<h4 id="downtime-without-a-reader-instance">Downtime without a reader instance</h4>
<p>If your writer instance restarts, you will get downtime. With a reader instance, this downtime can be minimised to seconds because the reader instance is promoted to take the place of the lost writer instance. Sometimes downtime with a reader instance is avoided all together, for example in scheduled maintenance.</p>
<p>If there's no reader instance, then downtime increases with database size. For even a small databases (just a few MB in size), it can require 10 minutes to restart. This restart time will increase with the size of the cluster.</p>
<h3 id="resources">Resources</h3>
<p>Aurora Serverless uses a unit of measure called the <code>ACU</code> or Aurora Capacity Units. An ACU is a combined unit of CPU, network and memory. One ACU is defined by AWS as being "roughly 2 GB of memory, and equivalent CPU and network bandwidth".</p>
<p>Beware of a really low minimum ACU, even if you have a low baseline workload. While your cluster may be able to service all it's normal traffic at just 0.5 of an ACU, this can cause problems when the cluster needs to scale. <strong>Aurora scales faster if it's current capacity is higher.</strong> If you have an instance running at 0.5 ACU and it receives a large spike in traffic, then those clusters could fail to scale fast enough so they run out of memory (or connections) and restart.</p>
<p>For production use, you should probably not have a minimum ACU below 2. With a single writer instance (i.e. no reader instance), you probably want more than that. This is because the ACU's are applied to each instance in the cluster. So if you have a minimum ACU of 2, then the writer and reader will each have 2 ACU's, providing an overall cluster capacity of 4 ACU's.</p>
<h2 id="what-about-aurora-v1">What about aurora V1</h2>
<p><strong>Aurora V1 should probably not be considered for production use</strong> (unless you are already running it).</p>
<p>The main benefit of using Aurora V2 is for auto scaling. Autoscaling in V1 is much slower and can potentially cause a DB crash and restart.</p>
<p>When it decides it needs to scale out, Aurora V1 has to find a scaling point. A scaling point is a quiet point in the database where it can make the required changes without interrupting any running transactions etc. Sometimes it can take a few minutes to find this scaling point. If Aurora V1 cannot find a scaling point in sufficient time, the DB will restart.</p>
<p>With Aurora V2, this scaling bottleneck does not exist, and Aurora can scale much more quickly.</p>
<h2 id="summary">Summary</h2>
<p>Before you upgrade your MySQL 5.7 instance, consider if MySQL is still the most appropriate technology to use</p>
<p>If MySQL is still a good choice for you, is an Aurora serverless V2 version a good choice, or would you be better off with a provisioned RDS instance instead.</p>
<p>If you do choose Aurora serverless V2, then do you need to have a reader instance in the cluster, or can you run with a single writer instance? What are the consequences of setting the minimum ACU's low?</p>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS CDK dependent stacks]]></title>
        <id>https://www.mydevnotebook.com/blog/2023/07/11/CDK-dependent-stacks</id>
        <link href="https://www.mydevnotebook.com/blog/2023/07/11/CDK-dependent-stacks"/>
        <updated>2023-07-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How to decouple dependent AWS stacks]]></summary>
        <content type="html"><![CDATA[<p>Sometimes you try deploying a change to an existing multi-stack CDK project, only to get an error message similar to "<strong>Export cannot be deleted as it is in use</strong>". Read on if you've encountered this error and are struggling to fix it, or you're just interested in finding out about dependent stacks in AWS.</p>
<h2 id="aws-cdk-dependent-stacks">AWS CDK dependent stacks</h2>
<p>AWS CDK makes it easy to share resources in different stacks. For example, allowing an SQS queue from one stack to be used by a lambda in a different stack. In this case, CDK automatically takes care of cross stack references, and also ensures deployments happen in the correct order.</p>
<p><img alt="CDK dependent stacks architecture" src="https://www.mydevnotebook.com/assets/images/DependentStacks-299bd51aa765cfff1a7e67cd60fde1f9.png" width="623" height="301"></p>
<p>The problem you've probably encountered seeing as you're here, is that you're trying to remove  that cross stack reference, but you see  something like:
<code>Export &lt;&lt;some AWS ref&gt;&gt; cannot be deleted as it is in use by &lt;&lt;some stack&gt;&gt;</code></p>
<h2 id="how-did-this-happen">How did this happen</h2>
<p>Cross stack references occur when you have two stacks, and you share something from one stack with the other. When CDK is building the cloudFormation template, it will automatically manage these cross stack references. It does this by "exporting" the construct from the producing stack (using an auto generated name), and "importing" the construct in the consuming construct.</p>
<p>If you make a change that impacts the cross stack dependency, you can find yourself in a circular trap where CDK knows it has to deploy the producingStack first, but that fails because the existing (already deployed) version of the consuming stack still requires the exports you're trying to remove, and it cannot deploy the consumingStack first because that depends on the producingStack.</p>
<p><img alt="CDK depent stacks deployment problem" src="https://www.mydevnotebook.com/assets/images/CDK-dependent-stacks-888d3a0f1147307a15cdb30fb1c11123.png" width="432" height="323"></p>
<h2 id="example">Example</h2>
<p>Given a stack <code>producingStack</code> that creates an SQS queue, and a <code>consumingStack</code> that has a lambda function that is allowed to push messages to the SQS queue, we end up with (partial) CDK code that looks a little like below (full example code can be found <a href="https://github.com/martynbutty/cdkDependantStacks">here</a>)</p>
<pre><code class="language-typeScript">...
export class producingStack extends cdk.Stack {  
    public readonly sourceQueueArn: string;  
    public readonly sourceQueueUrl: string;  

    constructor(scope: Construct, id: string, props?: cdk.StackProps) {  
        super(scope, id, props);  

        const sourceMessageQueue = new sqs.Queue(this, 'sourceMessageQueue', {  
            queueName: 'sourceMessageQueue'  
        });  

        this.sourceQueueArn = sourceMessageQueue.queueArn;  
        this.sourceQueueUrl = sourceMessageQueue.queueUrl;  
        ...
</code></pre>
<pre><code class="language-typescript">export class consumingStack extends cdk.Stack {  
    constructor(scope: Construct, id: string, props: HotelsDataAcquisitionStackProps) {  
        super(scope, id, props);  

        const queue = aws_sqs.Queue.fromQueueArn(this, 'sourceQ', props.sqsArn);  

        const eventSource = new SqsEventSource(queue, {  
            batchSize: 10,  
            reportBatchItemFailures : true,  
            enabled: true  
        });  

        const lambdaEnvVars = {  
            SQS_QUEUE_URL: props.sqsUrl  
        };  

        const processingLambda = new NodejsFunction(this, 'processingLambda', {  
            entry: './lambda/index.ts',  
            runtime: aws_lambda.Runtime.NODEJS_18_X,  
            handler: 'main',  
            environment: lambdaEnvVars  
        });  

        processingLambda.addEventSource(eventSource);  
    }  
}
</code></pre>
<p>We have already deployed these stacks, and are now trying to refactor our solution to remove the cross stack dependency.</p>
<h3 id="refactor-to-remove-a-dependency">Refactor to remove a dependency</h3>
<p>For this example, we're going to simply remove the SQS URL in the lambda's environment variable. producingStack is unchanged, and consumingStack looks like below.</p>
<blockquote>
<p>You'd normally just delete code that's no longer required, but it's shown commented out here to make it easier to see what's changing.</p>
</blockquote>
<pre><code class="language-typescript">export class consumingStack extends cdk.Stack {  
    constructor(scope: Construct, id: string, props: HotelsDataAcquisitionStackProps) {  
        super(scope, id, props);  

        const queue = aws_sqs.Queue.fromQueueArn(this, 'sourceQ', props.sqsArn);  

        const eventSource = new SqsEventSource(queue, {  
            batchSize: 10,  
            reportBatchItemFailures : true,  
            enabled: true  
        });  

        // const lambdaEnvVars = {  
        // 	SQS_QUEUE_URL: props.sqsUrl
        // };  
        
        const processingLambda = new NodejsFunction(this, 'processingLambda', {  
            entry: './lambda/index.ts',  
            runtime: aws_lambda.Runtime.NODEJS_18_X,  
            handler: 'main'
            // environment: lambdaEnvVars  
        });  

        processingLambda.addEventSource(eventSource);  
    }  
}
</code></pre>
<p>When you try to <code>cdk deploy</code> this change, you get a failure
`Stack Deployments Failed: Error: The stack named producingStack failed to deploy: UPDATE_ROLLBACK_COMPLETE</p>
<p>The cause of the rollback is output on the CLI output but it gets hidden at the end. To find it another way, you can use the AWS console -&gt; cloudformation -&gt; producingStack -&gt; events, where we see the offending error:
<code>Export producingStack:ExportsOutputRefsourceMessageQueueE741C4AF715E0816 cannot be deleted as it is in use by consumingStack</code></p>
<p>If we do a <code>cdk diff</code> we can see that an export is being removed</p>
<pre><code>...
Outputs
[-] Output ExportsOutputRefsourceMessageQueueE741C4AF715E0816: {"Value":{"Ref":"sourceMessageQueueE741C4AF"},"Export":{"Name":"producingStack:ExportsOutputRefsourceMessageQueueE741C4AF715E0816"}}
...
</code></pre>
<p>CDK automatically created this export when it first detected the cross stack dependency, and it is now attempting to remove it because of our change to consumingStack.</p>
<h4 id="what-went-wrong">What went wrong</h4>
<p>CDK knows that consumingStack depends on producingStack (for the SQS exports).  Any deploy will therefore trigger the deploy of the producingStack first, <strong>even if you try to just deploy the consumingStack</strong>.</p>
<p>In our case, we've only changed the consumingStack. This change means CDK notices we no longer need to "export" the SQS URL, so it removes that export, as can be seen in the above <code>cdk diff</code> partial output.</p>
<p>CDK tries to first apply the change to producingStack. This change is to remove the export of the SQS URL.  However, CDK notices that the currently deployed version of consumingStack still has an import on that URL (it hasn't deployed the change to the consumingStack yet). So CDK fails the deploy and rolls-back.</p>
<h3 id="the-fix">The fix</h3>
<p>To fix this problem, we make a temporary code change in <code>producingStack</code> to explicitly export a ref to the SQS URL. This is in addition to the decoupling change we already made in <code>consumingStack</code>. This export will therefore be in the cloudformation template regardless of if it is actually used or not. To achieve this, we use the AWS CDK <a href="https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.Stack.html#exportwbrvalueexportedvalue-options"><code>exportValue()</code></a>  function. In our case, we add the below to <code>producingStack</code>:</p>
<p><code>this.exportValue(sourceMessageQueue.queueUrl);</code></p>
<blockquote>
<p>Note: If you're observant, and wanted to completely decouple the stacks, we'd also have add another export as we also implicitly use the SQS ARN in the initial version. So you'd also add:</p>
</blockquote>
<p><code>this.exportValue(sourceMessageQueue.queueArn);</code></p>
<p>To confirm our change worked we can run another <code>cdk diff</code>. This time you should notice that the export is no longer removed (we know this by the absence of it's removal / negating output from the diff). CDK can therefore safely deploy <code>producingStack</code> without compromising the deployed (old) version of <code>consumingStack</code>. It then deploys the new version of <code>consumingStack</code> which removes the dependency.</p>
<p>Now we managed to deploy and remove the dependency, we can make a second change to remove the temporary exports (the <code>exportValue()</code>) and redeploy again to be in a clean state. This time a <code>cdk diff</code> should show the removal of the exports, but this is on now that <code>consumingStack</code> no longer depends on the export.</p>
<h2 id="useful-commands">Useful commands</h2>
<ul>
<li>Use <code>cdk diff</code> to check for removed "exports". If you see any, then retain them by manually exporting them with <code>exportValue()</code>.</li>
<li>To see if the currently deployed versions of your stacks have any exports: <code>aws cloudformation list-exports</code>. Note: you will probably always have some exports as the underlying CDK deployment framework usually creates a couple of exports.</li>
<li>You can see the exports locally in your <code>cdk.out</code> directory. For our example, look in <code>./dependantStacks/cdk.out/producingStack.template.json</code> for the <code>Outputs</code> section</li>
<li>You can see the exports in the <strong>currently deployed</strong> versions by viewing the cloudformation template in the AWS console -&gt; cloudformation -&gt; producingStack -&gt; template</li>
<li>Given an export name, you can see which stack(s) use it: <code>aws cloudformation list-imports --export-name producingStack:ExportsOutputRefsourceMessageQueueE741C4AF715E0816</code></li>
</ul>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance Testing Basics]]></title>
        <id>https://www.mydevnotebook.com/blog/2022/09/06/Performance-Testing-Basics</id>
        <link href="https://www.mydevnotebook.com/blog/2022/09/06/Performance-Testing-Basics"/>
        <updated>2022-09-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Quick introduction to the different types of performance tests]]></summary>
        <content type="html"><![CDATA[<p>Performance testing is more than just sending a large number of requests at a target system to see what happens.</p>
<p>Performance testing can help you to understand how many requests your system can handle, how long it takes to respond,
if a recent code change has improved or reduced performance and more. It can help you plan for when you need to scale
your application, define server resources like CPU, memory and disk space, and consider the introduction or improvement
of caching.</p>
<p><img alt="Performance Test Types" src="https://www.mydevnotebook.com/assets/images/performance_test_types-69e07a986b0ccae6a2fca744ce57a324.png" width="1073" height="674"></p>
<p>You may not need to have a full performance test suite straight away, so consider;</p>
<ul>
<li>What are you trying to test and why</li>
<li>Do you know what good looks like</li>
<li>What environments are available, and how much are they like production environments</li>
<li>For an API, which endpoints/calls will you test, and can you (or do you need to) vary each request to avoid testing the cache layer</li>
</ul>
<h2 id="slis-and-slos">SLI’s and SLO’s</h2>
<p>Service level indicators (SLI) and service level objectives (SLO) are a set of things you can define to describe what
good looks like, and then use as a benchmark for your performance tests. It will also help you understand the different
types of performance tests you should consider.</p>
<p><strong>SLI’s</strong> are things that are important to measure and report on, e.g. how long it takes to return a response to a
request, or the error rate (maybe as a percentage of all requests)</p>
<p><strong>SLO’s</strong> are objectives or target values for your service, of something that is measured by an SLI. e.g. a
response should be returned within 100 milliseconds.</p>
<p>It’s a good exercise to agree a set of SLI and or SLO with the team before you start performance testing.</p>
<h2 id="what-can-performance-tests-help-us-understand">What can performance tests help us understand</h2>
<ul>
<li>Slow response times</li>
<li>Long load times</li>
<li>Bottlenecks</li>
<li>Scalability problems (disk, CPU, memory, memory leaks, network)</li>
<li>Software problems</li>
<li>Software / Framework / Cache configuration problems</li>
<li>Insufficient resources</li>
</ul>
<h2 id="where-and-when-to-test">Where and When to test</h2>
<p>Running against your production system is probably a bad idea. As well as effecting your own production system, you
could potentially cause third party systems problems too!, not that I've ever inadvertently tested this theory ;)</p>
<p>If you use cloud hosting, you can probably create a production like system fairly easily.</p>
<p>If you cannot provision a production like environment to test against, you should resist “interpreting” the results to
scale them up or down accordingly to get an approximation of how your production service will behave; You cannot be
sure the hardware will scale linearly (e.g. you can’t rely on doubling CPU resulting in double the performance). You can
however keep a record of previous tests and gain insight into if recent changes have improved or regressed performance
on the test system. This could give you confidence to proceed to release to production.</p>
<p>You could run your performance tests against a single server or node in your production estate if you are confident you
won’t harm your service or business that it serves. This could be by targeting a single node or server that is out of
traffic.</p>
<p>You should capture and store the results of your performance tests so that you can track the performance results over time.</p>
<h2 id="types-of-performance-tests">Types of Performance Tests</h2>
<h2 id="smoke-tests">Smoke Tests</h2>
<p>Smoke tests are used to verify your service is functioning while only putting minimal load through the service.</p>
<h2 id="load-tests">Load Tests</h2>
<p>Load testing will test your service by simulating actual users of your system. This allows you to observe how things
like your database, code and hardware behave under load.
<img alt="Load Test" src="https://www.mydevnotebook.com/assets/images/load_test-3bb6ab78c897cd1e8a4ac30a0a5e4716.png" width="792" height="674"></p>
<p>This type of test should use expected load levels. It could be based on your SLO’s or non-functional requirements (NFR)’s.
You probably want to slowly ramp up requests from zero over a short time (half an hour)x§x  to “warm” the service under
test and its server. The expected load should then be sustained for at least a further one to two hours before another
gradual ramp down to zero requests.</p>
<h2 id="soak-tests">Soak Tests</h2>
<p>These are the same as load tests, except that they will run for a much longer period (twelve to twenty-four hours or
more). Soak tests will help you identify things like memory leaks which could be hard to detect over shorter term load tests.
<img alt="Soak Test" src="https://www.mydevnotebook.com/assets/images/soak_test-3dfbb5d68d7bbcfe3c43ffc28756c25c.png" width="1072" height="674"></p>
<h2 id="spike-tests">Spike Tests</h2>
<p>Spike tests again extend the basic load test. This time a short “spike” of traffic in excess of the expected load is
used to verify that your service can cope or recover from sudden large increases in load
<img alt="Spike Test" src="https://www.mydevnotebook.com/assets/images/spike_test-23cd36c88cd97267d824aa6cb26195e8.png" width="792" height="674"></p>
<h2 id="stress-tests">Stress Tests</h2>
<p>A stress test will start with zero load, and gradually increase until well over the maximum expected capacity of the
service. This type of testing allows you to plan for the future so your service is ready. This could help define things
like:</p>
<ul>
<li>defining auto-scaling</li>
<li>increasing server capacity (horizontal or vertical scaling)</li>
<li>introducing or improving caching</li>
<li>code changes (to optimise performance, remove bottle-necks etc)</li>
<li>and much more
<img alt="Stress Test" src="https://www.mydevnotebook.com/assets/images/stress_test-19b0a3e17c27064175210d10fe4fc29e.png" width="841" height="674"></li>
</ul>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS EC2 Instance Types]]></title>
        <id>https://www.mydevnotebook.com/blog/2022/02/14/AWS-EC2-Instance-Types</id>
        <link href="https://www.mydevnotebook.com/blog/2022/02/14/AWS-EC2-Instance-Types"/>
        <updated>2022-02-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How to understand AWS EC2 instance type codes]]></summary>
        <content type="html"><![CDATA[<p>To try and make it easier to select an instance type, AWS uses a set format for instance type naming.
This article will help you understand how AWS breaks down the instance type into families, generations,
capabilities and overall size.</p>
<p><img alt="EC2 Instance Codes" src="https://www.mydevnotebook.com/assets/images/AWS_EC2_codes-0fdf792367e80ea4d7cf8cbb39c9d2d6.png" width="356" height="191"></p>
<p>EC2 is Amazon Web Services’ (AWS) <strong>Elastic Compute Cloud</strong>.
As AWS has to support an almost endless set of possible workloads, they provide hundreds of different instance types.<br>
<!-- -->Having different instance types means you only use and pay for resources that you need,
rather than over provisioning and paying for unused resource.</p>
<blockquote>
<p>EC2 is an abbreviation for “elastic compute cloud”.
Where the same letter appears more than once, a count of that letter is used (e.g. two consecutive C's become C2)
Another example being S3 : Simple Storage Service - 3 S's</p>
</blockquote>
<p>Pretty much the only constant thing when working with AWS is that things change.
This article therefore is only meant to help you understand the EC2 instance code format rather than explaining every
possible instance type available.
You can see what instance types are available here: <a href="https://aws.amazon.com/ec2/instance-types/">Amazon EC2 Instance Types - Amazon Web Services</a></p>
<h2 id="instance-code-format">Instance code format</h2>
<p>As you can see from the above image, the instance code is divided into three or four sections (one section is optional).</p>
<h3 id="family">Family</h3>
<p>The first section is typically a single letter (but not always) and denotes the main instance type family.
The family lets you choose a type broadly suited to your tasks, and the most popular ones are discussed below (e.g. CPU optimised, memory optimised etc.)</p>
<h3 id="generation">Generation</h3>
<p>The higher the generation number the newer the instance type. You should usually choose the latest available
generation as that provides the newest features and generally can cost less. i.e. a newer generation might offer faster
CPU's, or more CPU, or more memory, or some other combination of features with better cost optimisation.
The exception to this rule might be if you only want to use the AWS free tier, as (at the time of writing)
the <code>t2.micro</code> is free tier eligible, but the <code>t3.micro</code> is not.</p>
<h3 id="additional-features">Additional Features</h3>
<p>This is an <strong>optional</strong> code. For example, <code>R6gd</code> instances offer
<em>local NVMe-based SSDs are physically connected to the host server and provide block-level storage that is coupled to the lifetime of the instance</em>,
where <code>R6g</code> is <em>EBS-Only storage</em>.</p>
<h3 id="instance-size">Instance Size</h3>
<p>The part after the period (dot) defines the instance size. This is usually a T-Shirt style size, but there are other options
too like <em>.metal</em> for example.</p>
<h2 id="popular-family-types">Popular Family Types</h2>
<h3 id="a---arm-processors">A - Arm Processors</h3>
<table><thead><tr><th></th><th>Description</th></tr></thead><tbody><tr><td>Characteristics</td><td>64-bit Arm cores using Graviton processors.</td></tr><tr><td>mnemonic</td><td><strong>a</strong> for Arm processor</td></tr><tr><td>example</td><td>a1.large : 2 CPU and 4 GiB Mem</td></tr></tbody></table>
<h3 id="t---general-purpose-burstable">T - General Purpose, Burstable</h3>
<table><thead><tr><th></th><th>Description</th></tr></thead><tbody><tr><td>Characteristics</td><td>T instances are general purpose that are “burstable”. If you use an “m” or “c” class type, you’re paying for the full capacity of the instance whether it’s in use or not. As a lot of workloads can have spikes of demand and longer periods of low demand, you can potentially save money by using a T type. T type machines have a baseline of capacity. When your system operates below the baseline, you earn CPU credits. When your system comes under high load, it can “burst” to use all the CPU by spending credits (or borrowing up to a days worth of credits depending on how you configure your instance).</td></tr><tr><td>mnemonic</td><td><strong>t</strong> for bursTable</td></tr><tr><td>example</td><td>t3.large : 2 CPU 8 GiB mem (36 CPU credits/hr)</td></tr></tbody></table>
<h3 id="m---general-purpose-medium">M - General Purpose, Medium</h3>
<table><thead><tr><th></th><th>Description</th></tr></thead><tbody><tr><td>Characteristics</td><td>General purpose machines optimised balance of CPU, memory and network performance</td></tr><tr><td>mnemonic</td><td><strong>m</strong> for medium</td></tr><tr><td>example</td><td>m5.large : 2 CPU 8 GiB mem</td></tr></tbody></table>
<h3 id="c---general-purpose-compute">C - General Purpose, Compute</h3>
<table><thead><tr><th></th><th>Description</th></tr></thead><tbody><tr><td>Characteristics</td><td>Optimised for compute intensive workloads.</td></tr><tr><td>mnemonic</td><td><strong>c</strong> for Compute</td></tr><tr><td>example</td><td>c5.large : 2 CPU 4 GiB mem</td></tr></tbody></table>
<h3 id="r---general-purpose-memory">R - General Purpose, Memory</h3>
<table><thead><tr><th></th><th>Description</th></tr></thead><tbody><tr><td>Characteristics</td><td>optimised for memory.</td></tr><tr><td>mnemonic</td><td><strong>r</strong> for RAM</td></tr><tr><td>example</td><td>r5.large : 2 CPU 16GiB mem</td></tr></tbody></table>
<h3 id="others">Others</h3>
<h4 id="accelerated-computing">Accelerated computing</h4>
<p>Optimised for machine learning, graphics, image processing etc. Some examples include the P[ictures], inf[erence] and g[raphics] types.</p>
<h4 id="storage">Storage</h4>
<p>Instances optimised for storage, i.e. where high sequential read and writes operations on large data sets is desirable
Examples include D[ense] (or D[ata]), I[ops], H[dd] based and others</p>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Docker build fail with VPN]]></title>
        <id>https://www.mydevnotebook.com/blog/2021/12/14/Docker-VPN-Network-Fix</id>
        <link href="https://www.mydevnotebook.com/blog/2021/12/14/Docker-VPN-Network-Fix"/>
        <updated>2021-12-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[A possible solution to docker build failures that require resources over a VPN]]></summary>
        <content type="html"><![CDATA[<p>Docker Desktop does a great job of taking care of all the networking for you and usually <em>“just works”</em>, even if you are connected to a corporate VPN.</p>
<p>Sometimes you may encounter difficulties which usually manifest as the <strong>build stage failing</strong>. This can happen when part of your build process requires resources that are only available when connected via your company VPN (e.g. internal package server).</p>
<p><img alt="Docker VPN" src="https://www.mydevnotebook.com/assets/images/Docker_VPN-06329a790d69c963ad27b786021526bf.png" width="685" height="301"></p>
<h2 id="the-problem">The Problem</h2>
<h3 id="tldr">Tl;dr</h3>
<p>Docker may have allocated an internal network range which clashes with your VPN.</p>
<h3 id="detailed-example">Detailed example</h3>
<p>Your project has dependancies on an internal server which might have an IP address of <code>172.17.2.79</code>. This is a <a href="https://en.wikipedia.org/wiki/Private_network">private network</a>) which is only accessible while connected to the companies private VPN.
Docker has created its own internal network which is using the  <code>172.17.0.0/16</code> range of addresses.
The build process tries to access your companies private package server at <code>172.17.2.79</code>.  The process is running within the private docker network. It therefore thinks the server should be available on the local docker network. It is also unable to send any traffic via the VPN due to the overlap or clash of the network range.</p>
<h2 id="confirming-this-is-the-problem">Confirming this is the problem</h2>
<p>To confirm the above scenario is indeed the root cause of the docker build failure, we need to compare the subnet docker is using against that in use by the VPN.
<strong>### Get Docker Network</strong>
First, list all the available docker networks</p>
<pre><code>$docker network ls
</code></pre>
<p>which should output something similar to</p>
<pre><code>NETWORK ID     NAME      DRIVER    SCOPE
ceb424d5d73d   bridge    bridge    local
ad6318517651   host      host      local
a2cfb19e8122   none      null      local
</code></pre>
<p>We’re interested in the “bridge” network, so we’ll inspect it by using it’s ID:</p>
<pre><code>$docker network inspect ceb424d5d73d
</code></pre>
<p>which should contain something like the following in its output</p>
<pre><code>"Config": [
    {
        "Subnet": "172.17.0.0/16"
    }
]
</code></pre>
<p>So we know that in the above case, docker is using the network range <code>172.17.0.0/16</code>
<strong>### Get VPN Network</strong>
Next we need to find out what network the VPN is using. The below should work on a MacBook Pro or linux. If you’re running on Windows, <code>ipconfig</code> may work better.
From a terminal enter the command <code>$ifconfig</code> or <code>$netstat -i</code> and look for a <code>utun*</code> entry with an IP address. For example, doing <code>$netstat -i</code> might output:</p>
<pre><code>utun3      1500  &lt;Link#17&gt;                      2298484     0   754726     0     0
utun3      1500  172.17.144/22 172.17.146.96    2298484     -   754726     -     -
</code></pre>
<p>This tells us the the VPN is using the network range <code>172.17.144/22</code>.
If you can remember your <a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv4_CIDR_blocks">CIDR blocks</a>, you’ll see that the docker network collides with that of the VPN. This means that docker will be unable to use the VPN network to get to our internal server which has the address <code>172.17.2.79</code>.</p>
<h2 id="docker-network-fix">Docker Network Fix</h2>
<p>To resolve the above problem, we can change the address pool used by the docker daemon.
With docker desktop, the easiest way to achieve this is to open the docker desktop dashboard (UI), click the “settings” gear cog in the top right of the screen to view the preferences screen. Select “Docker Engine” from the left hand menu, which should show some JSON config. Add the below section of config into the existing JSON config (keeping what’s already there), and click the “Apply &amp; Restart” button</p>
<pre><code>"default-address-pools": [
    {
      "base": "172.240.0.0/16",
      "size": 24
    }
  ],
</code></pre>
<p>Now if you repeat the steps to <a href="https://www.mydevnotebook.com/blog/2021/12/14/Docker-VPN-Network-Fix#Get-Docker-Network">Get Docker Network</a> (note, the network ID most likely changed!), you should see that docker now uses subnet <code>172.240.0.0/24</code>, and <code>docker build</code> should now be able to complete steps such as <code>dotnet build</code> which would have previously failed to reach the internal server</p>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer Memory Types]]></title>
        <id>https://www.mydevnotebook.com/blog/2021/11/06/memory</id>
        <link href="https://www.mydevnotebook.com/blog/2021/11/06/memory"/>
        <updated>2021-11-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[computer memory types and considerations from a tinyML perspective]]></summary>
        <content type="html"><![CDATA[<p>Here is an overview of some common types of computer memory. Although general in nature, this page is more about considerations
you may need to make when dealing with machine learning on a microcontroller running ML perspective (TinyML).</p>
<h2 id="what-is-it">What is it</h2>
<p>Computers operate on <strong>bits</strong> which can take the value zero or one.</p>
<p>To be more useful, these bits are grouped into <strong>bytes</strong>. One byte comprises 8 bits, which can then represent things like letters
or weights in a neural network (NN).</p>
<p>Microcontrollers and CPU's read and write bytes to memory.  A memory address is a hexadecimal value that tells the
cpu/microcontroller etc where the memory it is looking for is located.</p>
<h2 id="types-of-memory">Types of memory</h2>
<h3 id="flash">Flash</h3>
<p>Flash memory is non volatile (it won't lose the stored information when powered down). It is used to store program code,
machine learning (ML) model weights etc.</p>
<p>The process of saving to flash memory is slow and also gradually degrades the memory over time, therefore flash memory
is better suited to read only use, and only overwritten when reprogramming (a microcontroller)</p>
<h3 id="ram">RAM</h3>
<p>RAM is volatile (the stored information is lost when powered off). It is used for temporary storage of variables like
input and output buffers and intermediate tensors. RAM is much faster than flash for read/write operations, so is ideal
for use as primary memory during code execution</p>
<h3 id="types-of-ram">Types of RAM</h3>
<h4 id="dynamic-ram-dram">Dynamic RAM (DRAM)</h4>
<p>DRAM uses a single transistor and capacitor to store a bit. As the capacitor quickly loses charge, it must be periodically
refreshed to prevent the stored information being lost. This usually occurs between read and write operations. DRAM is
most suitable for main memory in modern computers.</p>
<h4 id="static-ram-sram">Static RAM (SRAM)</h4>
<p>SRAM uses six transistors to store each bit. It is able to maintain the stored information without refreshing. SRAM is
more expensive than DRAM on account of the number of transistors required, but is faster and requires less power due to
not needing to refresh. SRAM is therefore more suitable to caches, and is commonly used as main memory on microcontrollers.</p>
<h4 id="registers">Registers</h4>
<p>A third type of RAM is a register. A special purpose register is typically for low level computing functions like the program
counter and stack pointer. General purpose registers are used to store values and memory addresses. It is unlikely you
will need to play around with registers as a tinyML engineer unless you are working at the assembly code level.</p>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WSDL Glossary]]></title>
        <id>https://www.mydevnotebook.com/blog/2021/10/02/wsdl</id>
        <link href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl"/>
        <updated>2021-10-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[What is WSDL]]></summary>
        <content type="html"><![CDATA[<h2 id="what-is-wsdl">What is WSDL</h2>
<p>Web services description language (WSDL) is an XML document that describes a web service. WSDL is often pronounced "wizdul".
It describes what operations a web service provides, the structure of the messages it sends and receives, and how to send
those messages. This article is a very brief overview of the common WSDL elements. If you are new to WSDL, you may prefer
to read <a href="https://msdn.microsoft.com/en-us/library/ms996486.aspx" title="Understanding WSDL">Understanding WSDL</a>.</p>
<h2 id="core-wsdl-elements">Core WSDL Elements</h2>
<h3 id="definition">Definition</h3>
<p>The <code>&lt;definitions&gt;</code> element is the WSDL's root XML element. It typically contains several other elements including
<a href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl#types">types</a> , <a href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl#message">message</a>, <a href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl#portType">portType</a>, <a href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl#binding">binding</a> and <a href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl#service">service</a>.</p>
<h3 id="types">Types</h3>
<p>The <code>&lt;types&gt;</code> element contains XML schema type definitions (xsd). The xsd's describe the structure of the XML sent and
received by the web service.</p>
<h3 id="message">Message</h3>
<p>A message is an XML document that can be sent or received by the web service. A message is usually associated with one
or more <a href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl#operation">operation</a>. For example, an operation to <code>createNewOrder</code> might have an input message <code>newOrder</code>
and an output message <code>orderStatus</code>.</p>
<h3 id="porttype-interfac">PortType (Interfac</h3>
<p>The <code>&lt;portType&gt;</code> element is best thought of as an interface, and will be renamed as such in version 1.2 of the WSDL
specification. It contains one or more <code>&lt;operation&gt;</code> elements.</p>
<h3 id="operation">Operation</h3>
<p>An <code>&lt;operation&gt;</code> element defines an operation with the web service. It groups together the <a href="https://www.mydevnotebook.com/blog/2021/10/02/wsdl#message">message</a> elements
that can be passed to or from the web service. An operation can have an input and output message (request-response), or
it can just define an input message (one-way), send an output message only (notification) or send an output message to
ask for an input message (solicit-response). The operation may also define a fault message too.</p>
<h3 id="binding">Binding</h3>
<p>A <code>&lt;binding&gt;</code> element is a collection of one or more <code>operations</code>. It describes how an operation is implemented. It
defines the communication protocol (e.g. http), style of service (document or rpc), and the <code>SOAPAction</code> HTTP header for
the defined operations.</p>
<h3 id="document--rpc">Document &amp; RPC</h3>
<p>The document style indicates that the SOAP body will contain an XML document (and is able to be validated by the previously
defined xsd's). An RPC style indicates that the SOAP body will contain an XML representation of a method call. It includes
the method name and parameters of the method to generate the XML structure. Follow this link for a more in-depth discussion
on <a href="http://java.globinch.com/enterprise-java/web-services/soap-binding-document-rpc-style-web-services-difference/" title="Document vs RPC">Document and RPC style web services</a></p>
<h3 id="service">Service</h3>
<p>The <code>&lt;service&gt;</code> element defines the endpoint (port) that exposes a particular binding. I.e. the URL to use to call an
operation within the binding</p>
<h3 id="example-wsdl-document">Example WSDL Document</h3>
<pre><code class="language-xml">&lt;definitions name="EndorsementSearch"  
 targetNamespace="http://namespaces.snowboard-info.com"  
 xmlns:es="http://www.snowboard-info.com/EndorsementSearch.wsdl"  
 xmlns:esxsd="http://schemas.snowboard-info.com/EndorsementSearch.xsd"  
 xmlns:soap="http://schemas.xmlsoap.org/wsdl/soap/"  
 xmlns="http://schemas.xmlsoap.org/wsdl/"  
&gt;  
  
 &lt;!-- omitted types section with content model schema info \--&gt;  
  
 &lt;message name="GetEndorsingBoarderRequest"&gt;  
  &lt;part name="body" element="esxsd:GetEndorsingBoarder"/&gt;  
 &lt;/message&gt;  
  
 &lt;message name="GetEndorsingBoarderResponse"&gt;  
  &lt;part name="body" element="esxsd:GetEndorsingBoarderResponse"/&gt;  
 &lt;/message&gt;  
  
 &lt;portType name="GetEndorsingBoarderPortType"&gt;  
  &lt;operation name="GetEndorsingBoarder"&gt;  
   &lt;input message="es:GetEndorsingBoarderRequest"/&gt;  
   &lt;output message="es:GetEndorsingBoarderResponse"/&gt;  
   &lt;fault message="es:GetEndorsingBoarderFault"/&gt;  
  &lt;/operation&gt;  
 &lt;/portType&gt;  
  
 &lt;binding name="EndorsementSearchSoapBinding" type="es:GetEndorsingBoarderPortType"&gt;  
  &lt;soap:binding style="document" transport="http://schemas.xmlsoap.org/soap/http"/&gt;  
  &lt;operation name="GetEndorsingBoarder"&gt;  
   &lt;soap:operation soapAction="http://www.snowboard-info.com/EndorsementSearch"/&gt;  
   &lt;input&gt;  
    &lt;soap:body use="literal" 
                   namespace="http://schemas.snowboard-info.com/EndorsementSearch.xsd"/&gt;  
   &lt;/input&gt;  
   &lt;output&gt;  
    &lt;soap:body use="literal" 
                   namespace="http://schemas.snowboard-info.com/EndorsementSearch.xsd"/&gt;  
   &lt;/output&gt;  
   &lt;fault&gt;  
    &lt;soap:body use="literal" 
                   namespace="http://schemas.snowboard-info.com/EndorsementSearch.xsd"/&gt;  
   &lt;/fault&gt;  
  &lt;/operation&gt;  
 &lt;/binding&gt;  
  
 &lt;service name="EndorsementSearchService"&gt;  
  &lt;documentation&gt;snowboarding\-info.com Endorsement Service&lt;/documentation&gt;  
  &lt;port name="GetEndorsingBoarderPort" binding="es:EndorsementSearchSoapBinding"&gt;  
   &lt;soap:address location="http://www.snowboard-info.com/EndorsementSearch"/&gt;  
  &lt;/port&gt;  
 &lt;/service&gt;  
&lt;/definitions&gt;
</code></pre>]]></content>
        <author>
            <name>Martyn Butterworth</name>
            <uri>https://github.com/MartynButty</uri>
        </author>
    </entry>
</feed>