"use strict";(self.webpackChunkmydevnotebook_com=self.webpackChunkmydevnotebook_com||[]).push([[672],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return p}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function c(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var o=n.createContext({}),s=function(e){var t=n.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},d=function(e){var t=s(e.components);return n.createElement(o.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,d=c(e,["components","mdxType","originalType","parentName"]),u=s(a),p=r,m=u["".concat(o,".").concat(p)]||u[p]||h[p]||i;return a?n.createElement(m,l(l({ref:t},d),{},{components:a})):n.createElement(m,l({ref:t},d))}));function p(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=u;var c={};for(var o in t)hasOwnProperty.call(t,o)&&(c[o]=t[o]);c.originalType=e,c.mdxType="string"==typeof e?e:r,l[1]=c;for(var s=2;s<i;s++)l[s]=a[s];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},596:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return c},contentTitle:function(){return o},metadata:function(){return s},toc:function(){return d},default:function(){return u}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),l=["components"],c={title:"Caching Strategies (with a focus on microservices)",description:"caching strategies, topologies, eviction strategies etc from a microservice perspective"},o=void 0,s={unversionedId:"Architecture/caching",id:"Architecture/caching",title:"Caching Strategies (with a focus on microservices)",description:"caching strategies, topologies, eviction strategies etc from a microservice perspective",source:"@site/docs/Architecture/caching.md",sourceDirName:"Architecture",slug:"/Architecture/caching",permalink:"/Architecture/caching",tags:[],version:"current",frontMatter:{title:"Caching Strategies (with a focus on microservices)",description:"caching strategies, topologies, eviction strategies etc from a microservice perspective"},sidebar:"tutorialSidebar",previous:{title:"Architecture Notes",permalink:"/Architecture/Overview"},next:{title:"The Adapter Pattern",permalink:"/Design Patterns and Principles/adapter"}},d=[{value:"Topologies",id:"topologies",children:[{value:"Distributed",id:"distributed",children:[],level:3},{value:"Replicated",id:"replicated",children:[],level:3},{value:"Near Cache",id:"near-cache",children:[],level:3}],level:2},{value:"Comparing Topologies",id:"comparing-topologies",children:[],level:2},{value:"Spaced-based microservices",id:"spaced-based-microservices",children:[{value:"Reading from the database",id:"reading-from-the-database",children:[],level:3},{value:"Data Collisions",id:"data-collisions",children:[],level:3}],level:2},{value:"Cache eviction policies",id:"cache-eviction-policies",children:[{value:"TTL",id:"ttl",children:[],level:3},{value:"ARC",id:"arc",children:[],level:3},{value:"LFU",id:"lfu",children:[],level:3},{value:"LRU",id:"lru",children:[],level:3},{value:"RR",id:"rr",children:[],level:3}],level:2}],h={toc:d};function u(e){var t=e.components,c=(0,r.Z)(e,l);return(0,i.kt)("wrapper",(0,n.Z)({},h,c,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"topologies"},"Topologies"),(0,i.kt)("h3",{id:"distributed"},"Distributed"),(0,i.kt)("p",null,"A distributed cache is a client-server style cache. It is the cached data that is distributed, i.e. it's distributed between\nall the services that use it but there's only a single cache server/service."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"distributed cache diagram",src:a(5653).Z})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Uses a separate server (e.g. redis or ignite etc)"),(0,i.kt)("li",{parentName:"ul"},"All services using it use a client library to use the cache."),(0,i.kt)("li",{parentName:"ul"},"Each service does not have cached data; It has to access it from the server"),(0,i.kt)("li",{parentName:"ul"},"Not fault tolerant, unless using a cluster")),(0,i.kt)("p",null,"The cache can store the data as"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Type"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"IMDG"),(0,i.kt)("td",{parentName:"tr",align:null},"In Memory Data Grid - Simple")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"IMDB"),(0,i.kt)("td",{parentName:"tr",align:null},"In Memory DataBase - Can have a schema and thus be more complex, allows use of SQL like queries against the cached data")))),(0,i.kt)("h3",{id:"replicated"},"Replicated"),(0,i.kt)("p",null,'This type of caching topology requires no external server. The cached data resides in each service, in memory. The cache\nengine takes care of replicating the cached data amongst all instances, so it has "eventual consistency". Fewer products\noffer this type of caching compared to ',(0,i.kt)("a",{parentName:"p",href:"#Distributed"},"Distributed"),", including ",(0,i.kt)("a",{parentName:"p",href:"https://ignite.apache.org/"},"Ignite"),",\n",(0,i.kt)("a",{parentName:"p",href:"https://hazelcast.com/use-cases/caching/"},"Hazelcast"),", ",(0,i.kt)("a",{parentName:"p",href:"https://tanzu.vmware.com/gemfire"},"GemFire"),",\n",(0,i.kt)("a",{parentName:"p",href:"https://www.oracle.com/middleware/technologies/coherence.html"},"Coherence")," and\nothers."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"replicated cache diagram",src:a(1819).Z})),(0,i.kt)("p",null,'Eventual consistency is a by-product of how the cache operates. E.g. after a "put" onto the cache, the cache engine updates\nthe other instances in the background. Some systems allow concepts like ',(0,i.kt)("inlineCode",{parentName:"p"},"syncput()")," to force consistency, but this shouldn't\nusually be necessary. The data replication engine (e.g. ignite) maintains a socket level connection to the other instances."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Beware of your cache size! "),(0,i.kt)("p",{parentName:"blockquote"},"For example on a shared host running a kubernetes cluster, 30 instances each with a 200 MB cache requires 6 GB of data!.")),(0,i.kt)("p",null,"Similar to above, this topology is not viable if you have large and/or lots of storage and/or instances."),(0,i.kt)("h3",{id:"near-cache"},"Near Cache"),(0,i.kt)("p",null,"The near cache hybrid is a combination of a distributed and replicated cache. Each service has its own in memory cache,\nknown as the ",(0,i.kt)("strong",{parentName:"p"},"front cache")," (or MRU, or MFU cache depending on the eviction strategy used - see later). There is also a\ndistributed cache holding the full data set, known as the ",(0,i.kt)("strong",{parentName:"p"},"full backing cache"),". "),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"near cache diagram",src:a(2875).Z})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"There are no communications between services for cache management like in a replicated cache, instead this is managed by\nthe backing cache."),(0,i.kt)("li",{parentName:"ul"},"It allows a large amount of cached data in the full (backing) cache"),(0,i.kt)("li",{parentName:"ul"},"A smaller subset of data is stored in each service's front cache which might use MRU or MFU."),(0,i.kt)("li",{parentName:"ul"},"Individual front cache's can therefore contain different datasets to each other  "),(0,i.kt)("li",{parentName:"ul"},"It can suffer from inconsistent response times in some environments, especially in a microservice environment. This is\nbecause the data might not exist in the local front cache, so we have to go to the backing cache, and maybe even all the\nway to the database.\n")),(0,i.kt)("h2",{id:"comparing-topologies"},"Comparing Topologies"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null}),(0,i.kt)("th",{parentName:"tr",align:null},"Replicated"),(0,i.kt)("th",{parentName:"tr",align:null},"Distributed"),(0,i.kt)("th",{parentName:"tr",align:null},"Near Cache"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"optimisation"),(0,i.kt)("td",{parentName:"tr",align:null},"performance"),(0,i.kt)("td",{parentName:"tr",align:null},"consistency"),(0,i.kt)("td",{parentName:"tr",align:null},"balanced")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"cache Size"),(0,i.kt)("td",{parentName:"tr",align:null},"small (<100 MB)"),(0,i.kt)("td",{parentName:"tr",align:null},"large (500 MB +)"),(0,i.kt)("td",{parentName:"tr",align:null},"large (500 MB +)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"type of data"),(0,i.kt)("td",{parentName:"tr",align:null},"relatively static"),(0,i.kt)("td",{parentName:"tr",align:null},"transactional"),(0,i.kt)("td",{parentName:"tr",align:null},"relatively static")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"update frequency"),(0,i.kt)("td",{parentName:"tr",align:null},"relatively low"),(0,i.kt)("td",{parentName:"tr",align:null},"high"),(0,i.kt)("td",{parentName:"tr",align:null},"relatively low")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"fault tolerance"),(0,i.kt)("td",{parentName:"tr",align:null},"high"),(0,i.kt)("td",{parentName:"tr",align:null},"low"),(0,i.kt)("td",{parentName:"tr",align:null},"low")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"responsiveness"),(0,i.kt)("td",{parentName:"tr",align:null},"high"),(0,i.kt)("td",{parentName:"tr",align:null},"medium"),(0,i.kt)("td",{parentName:"tr",align:null},"variable")))),(0,i.kt)("h2",{id:"spaced-based-microservices"},"Spaced-based microservices"),(0,i.kt)("p",null,"A challenge of microservices, particularly in migrating to them from a monolith, is how each service has its own data and\ndoes not require access to a large central shared database. If there were many services sharing the same database, attempting\nto make a schema change to that database would require very careful choreography to prevent breaking any services (or more\nlikely, you simply wouldn't be able to make the change without breaking something)."),(0,i.kt)("p",null,"With caching you can share a single database in a microservice environment. You can even version database changes, increase\nperformance, use cloud based data sync but on-prem database, and still have simple reporting and data analytics."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"spaced based cache diagram",src:a(8348).Z})),(0,i.kt)("p",null,"All data is stored in memory using replicated caches. Each service's cache is a sub-set of the data in the database, but\na full data set for that particular services needs. Even if you have large database that is not practical to cache (e.g.\n2 TB), the data required for an individual microservice might only be 200 MB of that 2 TB."),(0,i.kt)("h3",{id:"reading-from-the-database"},"Reading from the database"),(0,i.kt)("p",null,"Reads from the database can happen in the following circumstances"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"An external application updates the database. An async data writer will (eventually) update the cache data in this case"),(0,i.kt)("li",{parentName:"ul"},"Cold starts - i.e. no instance of the service is already running from which to clone the data. This could be because\nwe lost all instances, or needed to perform a schema change or redeploy etc"),(0,i.kt)("li",{parentName:"ul"},"Archiving of data (so cache needs renewing/refreshing)")),(0,i.kt)("h3",{id:"data-collisions"},"Data Collisions"),(0,i.kt)("p",null,"When two service instances are operating at the same time, data collisions can occur. For example in an inventory\nsystem, both instance caches might have the same stock level for an item, and both perform separate operations to decrement\nthe stock by different amounts (e.g. for different orders) ",(0,i.kt)("em",{parentName:"p"},"at the same time"),". This means the service that completes last,\noverwrites the change made by the first services operation. "),(0,i.kt)("p",null,"This can be mitigated by not updating local caches but by performing a put operation to a queue. An ",(0,i.kt)("inlineCode",{parentName:"p"},"inventory adjust")," service\ncould then be created to read from this queue, apply each update in turn to itself and its own cache which would then replicate\nto the other services as normal. The trade-off is the time to eventual consistency."),(0,i.kt)("h2",{id:"cache-eviction-policies"},"Cache eviction policies"),(0,i.kt)("h3",{id:"ttl"},"TTL"),(0,i.kt)("p",null,"Time to live, is a simple timeout parameter. It doesn't address a full cache scenario"),(0,i.kt)("h3",{id:"arc"},"ARC"),(0,i.kt)("p",null,"Archive policy (ARC) evicts item based on date created (NOT when it was added to the cache). For example evict all items\nolder than 6 months. It doesn't address a full cache scenario"),(0,i.kt)("h3",{id:"lfu"},"LFU"),(0,i.kt)("p",null,'Least frequently used. Items have an internal "count" of how often it is read. A put operation must reset this internal\ncount for all items (otherwise the item just added would most likely be evicted again), so this can be suboptimal.'),(0,i.kt)("h3",{id:"lru"},"LRU"),(0,i.kt)("p",null,'Least recently used. All items get an "age". Oldest item gets evicted on put. On cache item hit, that individual items age\nis set to zero and all other items ages are incremented. This is more effective than LFU, but more expensive because of\nthe re-aging process.'),(0,i.kt)("h3",{id:"rr"},"RR"),(0,i.kt)("p",null,"Random replacement. If doing a put on a full cache, then randomly evict an item. This can be a good initial strategy if\nyou don't know much about you data / hotspots, or for when data access is inconsistent or non-deterministic."))}u.isMDXComponent=!0},5653:function(e,t,a){t.Z=a.p+"assets/images/distributed_cache-5a2b7a3d6e54686e6c7c0167257ca60d.png"},2875:function(e,t,a){t.Z=a.p+"assets/images/near_cache-b74a392a60c595ba2216d1bef2191a6d.png"},1819:function(e,t,a){t.Z=a.p+"assets/images/replicated_cache-f986aa7b4d7dc11d4ad5c45ba5b5c8b5.png"},8348:function(e,t,a){t.Z=a.p+"assets/images/space_based-8aa326f1f7da72a2f206508d981365b3.png"}}]);